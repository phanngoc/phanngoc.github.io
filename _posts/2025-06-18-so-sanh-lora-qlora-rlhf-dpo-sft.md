---
layout: post
title: "So s√°nh c√°c ph∆∞∆°ng ph√°p LoRA, QLoRA, RLHF, DPO, SFT: G√≥c nh√¨n to√°n h·ªçc v√† ·ª©ng d·ª•ng th·ª±c t·∫ø"
date: 2025-01-18 10:00:00 +0700
categories: [machine-learning, llm, fine-tuning]
tags: [LoRA, QLoRA, RLHF, DPO, SFT, LLM, fine-tuning, deep-learning]
math: true
---

Trong vi·ªác hu·∫•n luy·ªán v√† fine-tuning c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM), ch√∫ng ta c√≥ nhi·ªÅu ph∆∞∆°ng ph√°p kh√°c nhau. Vi·ªác hi·ªÉu r√µ **to√°n h·ªçc** ƒë·∫±ng sau m·ªói ph∆∞∆°ng ph√°p v√† **use case th·ª±c t·∫ø** s·∫Ω gi√∫p b·∫°n l·ª±a ch·ªçn k·ªπ thu·∫≠t ph√π h·ª£p nh·∫•t cho t·ª´ng t√¨nh hu·ªëng c·ª• th·ªÉ.

## üîç **T·ªïng quan so s√°nh**

| Ph∆∞∆°ng ph√°p | M·ª•c ti√™u ch√≠nh | To√°n h·ªçc c·ªët l√µi | D·ªØ li·ªáu ƒë·∫ßu v√†o | K·ªπ thu·∫≠t tr·ªçng t√¢m | ∆Øu ƒëi·ªÉm ch√≠nh | Use case ƒëi·ªÉn h√¨nh |
|-------------|---------------|------------------|-----------------|-------------------|---------------|--------------------|
| **SFT** (Supervised Fine-Tuning) | D·∫°y m√¥ h√¨nh d·ª±a v√†o c·∫∑p (prompt, output) ƒë√∫ng | Cross-entropy loss tr√™n d·ªØ li·ªáu g√°n nh√£n | Prompt + Ground truth | Fine-tune m√¥ h√¨nh | D·ªÖ th·ª±c hi·ªán, ch·∫•t l∆∞·ª£ng cao n·∫øu c√≥ d·ªØ li·ªáu chu·∫©n | GPT for code, Q&A, translation |
| **RLHF** (Reinforcement Learning from Human Feedback) | T·ªëi ∆∞u h√†nh vi m√¥ h√¨nh d·ª±a v√†o ƒë√°nh gi√° ng∆∞·ªùi d√πng | PPO (Proximal Policy Optimization), reward model | Prompt + c√°c ph·∫£n h·ªìi v√† ranking | Fine-tune b·∫±ng RL tr√™n reward model | C·∫£i thi·ªán alignment v·ªõi ng∆∞·ªùi d√πng | ChatGPT, Claude |
| **DPO** (Direct Preference Optimization) | Tr·ª±c ti·∫øp t·ªëi ∆∞u h√≥a theo ranking ng∆∞·ªùi d√πng, kh√¥ng c·∫ßn reward model | Margin-based loss gi·ªØa output t·ªët v√† x·∫•u | Prompt + (output t·ªët, output x·∫•u) | Contrastive loss | ƒê∆°n gi·∫£n h∆°n RLHF, kh√¥ng c·∫ßn reward model | Thay RLHF cho c√°c LLM nh·ªè |
| **LoRA** (Low-Rank Adaptation) | Fine-tune m√¥ h√¨nh hi·ªáu qu·∫£ b·∫±ng c√°ch th√™m c√°c adapter | Approximate full weight update: ŒîW ‚âà A¬∑B·µÄ v·ªõi rank th·∫•p | Same as SFT/RLHF | Low-rank matrices + freezing base model | Gi·∫£m s·ªë param c·∫ßn update | Fine-tune LLM nhanh, r·∫ª |
| **QLoRA** (Quantized LoRA) | K·∫øt h·ª£p LoRA v·ªõi m√¥ h√¨nh l∆∞·ª£ng t·ª≠ h√≥a (quantized) | LoRA + Quantization-aware training | Same as SFT/RLHF | 4-bit quantization + LoRA | Si√™u nh·∫π, train tr√™n 1 GPU | Fine-tune LLM 65B b·∫±ng laptop/GPU nh·ªè |

---

## üß† 1. **SFT ‚Äì Supervised Fine-Tuning**

### üßÆ **To√°n h·ªçc:**

SFT l√† ph∆∞∆°ng ph√°p c∆° b·∫£n nh·∫•t, t·ªëi ∆∞u h√≥a h√†m m·∫•t m√°t cross-entropy:

$$L_{\text{SFT}} = - \sum_{t=1}^T \log P_\theta(y_t | x, y_{<t})$$

Trong ƒë√≥:
- $x$ l√† input prompt
- $y_t$ l√† token th·ª© t trong output
- $y_{<t}$ l√† c√°c token tr∆∞·ªõc ƒë√≥
- $\theta$ l√† tham s·ªë m√¥ h√¨nh

### ‚úÖ **Use case:**

- **Khi n√†o d√πng**: Khi b·∫°n c√≥ d·ªØ li·ªáu chu·∫©n (v√≠ d·ª•: input ‚Üí output ƒë√∫ng)
- **·ª®ng d·ª•ng th·ª±c t·∫ø**: 
  - Code generation (nh∆∞ GitHub Copilot)
  - Question-Answering systems
  - Machine translation
  - Text summarization

### üí° **V√≠ d·ª• th·ª±c t·∫ø:**

```python
# Pseudo-code cho SFT
def sft_training(model, dataset):
    for batch in dataset:
        inputs, targets = batch
        logits = model(inputs)
        loss = cross_entropy_loss(logits, targets)
        loss.backward()
        optimizer.step()
```

---

## üß† 2. **RLHF ‚Äì Reinforcement Learning from Human Feedback**

### üßÆ **To√°n h·ªçc:**

RLHF g·ªìm 3 giai ƒëo·∫°n:

1. **SFT**: Hu·∫•n luy·ªán supervised ban ƒë·∫ßu
2. **Train reward model** $R(x, y)$ t·ª´ c·∫∑p ranking: $y_{\text{better}} > y_{\text{worse}}$
3. **RL fine-tuning** b·∫±ng PPO:

$$\mathcal{L}_{\text{PPO}}(\theta) = \mathbb{E} \left[ \min\left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A, \text{clip}\left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) A \right) \right]$$

Trong ƒë√≥:
- $A$ l√† advantage function d·ª±a tr√™n reward
- $\epsilon$ l√† clipping parameter (th∆∞·ªùng l√† 0.2)
- $\pi_\theta$ l√† policy hi·ªán t·∫°i

### ‚úÖ **Use case:**

- **Khi n√†o d√πng**: Khi mu·ªën m√¥ h√¨nh "th√¢n thi·ªán" v√† aligned v·ªõi con ng∆∞·ªùi
- **·ª®ng d·ª•ng th·ª±c t·∫ø**:
  - ChatGPT, Claude
  - Chatbots customer service
  - Creative writing assistants
  - Educational AI tutors

### üí° **V√≠ d·ª• th·ª±c t·∫ø:**

```python
# Pseudo-code cho RLHF
def rlhf_training(model, reward_model, dataset):
    # Stage 1: SFT
    sft_training(model, dataset)
    
    # Stage 2: Train reward model
    reward_model = train_reward_model(comparison_dataset)
    
    # Stage 3: PPO training
    for episode in range(num_episodes):
        prompt = sample_prompt()
        response = model.generate(prompt)
        reward = reward_model(prompt, response)
        ppo_update(model, reward)
```

---

## üß† 3. **DPO ‚Äì Direct Preference Optimization**

### üßÆ **To√°n h·ªçc:**

DPO tr·ª±c ti·∫øp t·ªëi ∆∞u x√°c su·∫•t ch·ªçn ra m·∫´u t·ªët h∆°n:

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y^+,y^-) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y^+|x)}{\pi_{\text{ref}}(y^+|x)} - \beta \log \frac{\pi_\theta(y^-|x)}{\pi_{\text{ref}}(y^-|x)} \right) \right]$$

Trong ƒë√≥:
- $y^+$ l√† response ƒë∆∞·ª£c prefer
- $y^-$ l√† response k√©m h∆°n
- $\beta$ l√† hyperparameter ƒëi·ªÅu ch·ªânh "temperature"
- $\pi_{\text{ref}}$ l√† reference model (th∆∞·ªùng l√† SFT model)
- $\sigma$ l√† sigmoid function

### ‚úÖ **Use case:**

- **Khi n√†o d√πng**: Thay th·∫ø RLHF trong tr∆∞·ªùng h·ª£p t√†i nguy√™n h·∫°n ch·∫ø
- **·ª®ng d·ª•ng th·ª±c t·∫ø**:
  - Open-source LLMs (Mistral, OpenChat)
  - Zephyr, Starling models
  - Khi c·∫ßn training pipeline ƒë∆°n gi·∫£n h∆°n RLHF

### üí° **V√≠ d·ª• th·ª±c t·∫ø:**

```python
# Pseudo-code cho DPO
def dpo_training(model, preference_dataset, beta=0.1):
    for batch in preference_dataset:
        prompts, preferred, rejected = batch
        
        # Get log probabilities
        log_pi_preferred = model.log_prob(preferred, prompts)
        log_pi_rejected = model.log_prob(rejected, prompts)
        
        # DPO loss
        logits = beta * (log_pi_preferred - log_pi_rejected)
        loss = -torch.nn.functional.logsigmoid(logits).mean()
        
        loss.backward()
        optimizer.step()
```

---

## üß† 4. **LoRA ‚Äì Low-Rank Adaptation**

### üßÆ **To√°n h·ªçc:**

LoRA g√°n bi·∫øn th·ªÉ tr·ªçng s·ªë nh∆∞ sau:

$$W' = W + \Delta W, \quad \Delta W = A B^T$$

Trong ƒë√≥:
- $A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{k \times r}$, v·ªõi $r \ll d, k$
- $r$ l√† rank (th∆∞·ªùng t·ª´ 4-64)
- Ch·ªâ update $A$ v√† $B$, c√≤n $W$ (tr·ªçng s·ªë g·ªëc) ƒë∆∞·ª£c gi·ªØ nguy√™n (frozen)

**S·ªë tham s·ªë c·∫ßn train**: $r \times (d + k)$ thay v√¨ $d \times k$

### ‚úÖ **Use case:**

- **Khi n√†o d√πng**: Fine-tune m√¥ h√¨nh r·∫•t l·ªõn v·ªõi chi ph√≠ th·∫•p
- **·ª®ng d·ª•ng th·ª±c t·∫ø**:
  - Fine-tune LLaMA, Falcon cho domain-specific tasks
  - Personalized AI assistants
  - Multi-task learning v·ªõi nhi·ªÅu LoRA adapters

### üí° **V√≠ d·ª• th·ª±c t·∫ø:**

```python
# Pseudo-code cho LoRA
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, original_layer, rank=16):
        super().__init__()
        self.original_layer = original_layer
        self.rank = rank
        
        # Freeze original weights
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # LoRA matrices
        self.lora_A = nn.Parameter(torch.randn(rank, original_layer.in_features))
        self.lora_B = nn.Parameter(torch.zeros(original_layer.out_features, rank))
        
    def forward(self, x):
        # Original output + LoRA adaptation
        return self.original_layer(x) + (x @ self.lora_A.T @ self.lora_B.T)
```

---

## üß† 5. **QLoRA ‚Äì Quantized LoRA**

### üßÆ **To√°n h·ªçc:**

QLoRA k·∫øt h·ª£p:
- **4-bit quantization** c·ªßa tr·ªçng s·ªë c∆° s·ªü $W$
- **LoRA adaptation** nh∆∞ tr√™n

K·ªπ thu·∫≠t ch√≠nh:
- **NF4 quantization**: Quantization scheme t·ªëi ∆∞u cho neural networks
- **Double quantization**: Quantize c·∫£ quantization constants
- **Paged optimizers**: Qu·∫£n l√Ω memory hi·ªáu qu·∫£

$$W_{\text{quantized}} = \text{Quantize}(W, 4\text{-bit}) + A B^T$$

### ‚úÖ **Use case:**

- **Khi n√†o d√πng**: Fine-tune m√¥ h√¨nh l·ªõn tr√™n GPU 24GB tr·ªü xu·ªëng
- **·ª®ng d·ª•ng th·ª±c t·∫ø**:
  - Fine-tune LLaMA 65B tr√™n single GPU
  - Guanaco, Zephyr models
  - Research v·ªõi budget h·∫°n ch·∫ø

### üí° **V√≠ d·ª• th·ª±c t·∫ø:**

```python
# Pseudo-code cho QLoRA
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

# Quantization config
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    quantization_config=quantization_config
)

# Apply LoRA
model = get_peft_model(model, lora_config)
```

---

## üìä **So s√°nh hi·ªáu qu·∫£ t√†i nguy√™n**

| Ph∆∞∆°ng ph√°p | GPU Memory (7B model) | Training Time | S·ªë parameters c·∫ßn update |
|-------------|----------------------|---------------|-------------------------|
| **Full Fine-tuning** | ~28GB | 100% | 7B (100%) |
| **LoRA (r=16)** | ~16GB | 50% | ~4M (0.057%) |
| **QLoRA (r=16)** | ~6GB | 70% | ~4M (0.057%) |
| **DPO** | ~28GB | 80% | 7B (100%) |
| **RLHF** | ~56GB | 300% | 7B + Reward Model |

---

## üìå **T√≥m t·∫Øt l·ª±a ch·ªçn theo t√¨nh hu·ªëng**

| T√¨nh hu·ªëng | G·ª£i √Ω k·ªπ thu·∫≠t | L√Ω do |
|------------|---------------|-------|
| C√≥ d·ªØ li·ªáu g√°n nh√£n chu·∫©n | ‚úÖ **SFT** | ƒê∆°n gi·∫£n, hi·ªáu qu·∫£, baseline t·ªët |
| Mu·ªën m√¥ h√¨nh th√¢n thi·ªán h∆°n v·ªõi ng∆∞·ªùi d√πng | ‚úÖ **RLHF** ho·∫∑c **DPO** | Alignment v·ªõi human preferences |
| Kh√¥ng c√≥ reward model, c·∫ßn d·ªÖ implement | ‚úÖ **DPO** | ƒê∆°n gi·∫£n h∆°n RLHF, kh√¥ng c·∫ßn 3 stages |
| √çt t√†i nguy√™n, c·∫ßn fine-tune m√¥ h√¨nh l·ªõn | ‚úÖ **LoRA** ho·∫∑c **QLoRA** | Gi·∫£m memory v√† compute requirements |
| Ch·∫°y tr√™n laptop, GPU 1-card | ‚úÖ **QLoRA** | Si√™u nh·∫π, ph√π h·ª£p consumer hardware |
| C·∫ßn fine-tune nhi·ªÅu tasks kh√°c nhau | ‚úÖ **LoRA** | C√≥ th·ªÉ swap nhi·ªÅu adapters |
| Production v·ªõi budget cao | ‚úÖ **RLHF** | Ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t cho user experience |

---

## üîó **K·∫øt h·ª£p c√°c ph∆∞∆°ng ph√°p**

Trong th·ª±c t·∫ø, c√°c ph∆∞∆°ng ph√°p n√†y th∆∞·ªùng ƒë∆∞·ª£c k·∫øt h·ª£p:

1. **SFT ‚Üí DPO**: Ph·ªï bi·∫øn nh·∫•t
2. **SFT ‚Üí LoRA ‚Üí DPO**: Cho resource-constrained environments
3. **SFT ‚Üí RLHF**: Classic approach (ChatGPT style)
4. **QLoRA + DPO**: Cho research v√† small teams

---

## üìö **T√†i li·ªáu tham kh·∫£o**

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

---

**K·∫øt lu·∫≠n**: Vi·ªác l·ª±a ch·ªçn ph∆∞∆°ng ph√°p ph√π h·ª£p ph·ª• thu·ªôc v√†o t√†i nguy√™n, d·ªØ li·ªáu, v√† m·ª•c ti√™u c·ª• th·ªÉ c·ªßa b·∫°n. V·ªõi s·ª± ph√°t tri·ªÉn nhanh ch√≥ng c·ªßa lƒ©nh v·ª±c n√†y, DPO v√† QLoRA ƒëang tr·ªü th√†nh nh·ªØng l·ª±a ch·ªçn ph·ªï bi·∫øn nh·ªù t√≠nh hi·ªáu qu·∫£ v√† d·ªÖ tri·ªÉn khai.

B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu v·ªõi **SFT + LoRA** ƒë·ªÉ l√†m quen, sau ƒë√≥ chuy·ªÉn sang **DPO** khi c·∫ßn c·∫£i thi·ªán alignment, ho·∫∑c **QLoRA** khi t√†i nguy√™n h·∫°n ch·∫ø.
