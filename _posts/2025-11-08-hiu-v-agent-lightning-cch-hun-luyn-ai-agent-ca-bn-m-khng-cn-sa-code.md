---
layout: "post"
title: "Hi·ªÉu V·ªÅ Agent Lightning: C√°ch Hu·∫•n Luy·ªán AI Agent C·ªßa B·∫°n M√† Kh√¥ng C·∫ßn S·ª≠a Code"
date: "2025-11-08 09:51:54 +0700"
tags: ["agent-lightning", "llm", "rl"]
math: true
---

## Gi·ªõi thi·ªáu: V·∫•n ƒë·ªÅ m√† ch√∫ng ta ƒëang gi·∫£i quy·∫øt

B·∫°n ƒë√£ bao gi·ªù x√¢y d·ª±ng m·ªôt AI agent s·ª≠ d·ª•ng LangChain, AutoGen, hay OpenAI Agents SDK ch∆∞a? N·∫øu c√≥, b·∫°n c√≥ th·ªÉ ƒë√£ g·∫∑p ph·∫£i m·ªôt c√¢u h·ªèi quan tr·ªçng: **L√†m th·∫ø n√†o ƒë·ªÉ agent c·ªßa t√¥i h·ªçc h·ªèi v√† c·∫£i thi·ªán t·ª´ nh·ªØng tr·∫£i nghi·ªám th·ª±c t·∫ø?**[1][2]

Tr∆∞·ªõc ƒë√¢y, n·∫øu mu·ªën t·ªëi ∆∞u h√≥a m·ªôt AI agent b·∫±ng Reinforcement Learning (RL), b·∫°n ph·∫£i:
- Vi·∫øt l·∫°i to√†n b·ªô agent code ƒë·ªÉ t√≠ch h·ª£p v·ªõi h·ªá th·ªëng training[2][1]
- S·ª≠ d·ª•ng sequence concatenation v·ªõi masking ph·ª©c t·∫°p[1][2]
- G·∫Øn ch·∫∑t logic agent v·ªõi training pipeline, khi·∫øn vi·ªác maintain tr·ªü n√™n kh√≥ khƒÉn[3][1]

**Agent Lightning** t·ª´ Microsoft Research ƒë√£ thay ƒë·ªïi ho√†n to√†n game n√†y. ƒê√¢y l√† framework ƒë·∫ßu ti√™n cho ph√©p b·∫°n hu·∫•n luy·ªán **B·∫§T K·ª≤ AI agent n√†o** v·ªõi RL m√† **h·∫ßu nh∆∞ KH√îNG C·∫¶N s·ª≠a code**.[4][5][6][2][1]

***

## Agent Lightning l√† g√¨?

Agent Lightning l√† m·ªôt framework linh ho·∫°t v√† m·ªü r·ªông cho ph√©p hu·∫•n luy·ªán Large Language Models (LLMs) cho b·∫•t k·ª≥ AI agent n√†o th√¥ng qua Reinforcement Learning. ƒêi·ªÉm ƒë·∫∑c bi·ªát l√† n√≥ ƒë·∫°t ƒë∆∞·ª£c **s·ª± t√°ch bi·ªát ho√†n to√†n** (complete decoupling) gi·ªØa vi·ªác th·ª±c thi agent v√† qu√° tr√¨nh training.[7][2][3][1]

### T√≠nh nƒÉng n·ªïi b·∫≠t

üîå **Plug-and-Play v·ªõi m·ªçi framework**: H·ªó tr·ª£ LangChain, OpenAI Agents SDK, AutoGen, CrewAI, ho·∫∑c th·∫≠m ch√≠ agent t·ª± vi·∫øt t·ª´ ƒë·∫ßu[5][4][7][1]

üí§ **G·∫ßn nh∆∞ kh√¥ng c·∫ßn s·ª≠a code**: Ch·ªâ c·∫ßn th√™m v√†i d√≤ng code ƒë·ªÉ enable tracing[8][4][5]

üéØ **T·ªëi ∆∞u h√≥a c√≥ ch·ªçn l·ªçc**: C√≥ th·ªÉ ch·ªçn train m·ªôt ho·∫∑c nhi·ªÅu agents trong h·ªá th·ªëng multi-agent[4][7][5]

ü§ó **Nhi·ªÅu thu·∫≠t to√°n**: H·ªó tr·ª£ Reinforcement Learning (PPO, GRPO), Automatic Prompt Optimization, Supervised Fine-tuning v√† nhi·ªÅu h∆°n n·ªØa[9][10][11][4]

---

## Ki·∫øn tr√∫c c·ªët l√µi c·ªßa Agent Lightning

### 1. Training-Agent Disaggregation Architecture

ƒê√¢y l√† ki·∫øn tr√∫c quan tr·ªçng nh·∫•t gi√∫p t√°ch bi·ªát agent execution kh·ªèi RL training. H·ªá th·ªëng bao g·ªìm hai th√†nh ph·∫ßn ch√≠nh:[2][3][1]

#### **Lightning Server** (Teacher/Coach)
- ƒêi·ªÅu khi·ªÉn qu√° tr√¨nh RL training[3][1]
- Qu·∫£n l√Ω training process v√† version resources[12][3]
- Expose OpenAI-compatible API ƒë·ªÉ agents c√≥ th·ªÉ g·ªçi model ƒë√£ ƒë∆∞·ª£c update[6][1][3]
- Ch·∫°y tr√™n GPU clusters ƒë·ªÉ train models[8][12]

#### **Lightning Client** (Student/Runner)
- Ch·∫°y agent v√† th·ª±c hi·ªán data collection[1][3]
- Handle communication v·ªõi server ƒë·ªÉ truy·ªÅn v√† nh·∫≠n d·ªØ li·ªáu[3][1]
- Ho·∫°t ƒë·ªông nh∆∞ agent runtime, transparently qu·∫£n l√Ω agent execution m√† kh√¥ng c·∫ßn s·ª≠a code[1][3]
- C√≥ th·ªÉ ch·∫°y tr√™n CPU machines, scale ƒë·ªôc l·∫≠p v·ªõi training[12][8]

**Workflow diagram ƒë·∫ßy ƒë·ªß:**

```mermaid
graph TB
    subgraph "Lightning Client - Agent Runtime"
        A[Your Agent Code<br/>LangChain/AutoGen/OpenAI SDK]
        B[Tracer/Instrumentation<br/>OpenTelemetry Spans]
        C[Data Collection<br/>Captures LLM calls, tools, rewards]
    end
    
    subgraph "Communication Layer"
        D[Lightning Client API<br/>Send traces & rewards]
    end
    
    subgraph "Lightning Server - Training System"
        E[Task Queue<br/>Manage rollout tasks]
        F[Lightning Store<br/>Store execution traces]
        G[Training Engine<br/>VERL/PPO/GRPO]
        H[Model Registry<br/>Version control]
    end
    
    subgraph "Updated Model"
        I[OpenAI-Compatible API<br/>Serve optimized model]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I -.Calls model.-> A
    
    style A fill:#e1f5ff
    style G fill:#ffe1e1
    style I fill:#e1ffe1
```

**Gi·∫£i th√≠ch workflow:**
1. Agent c·ªßa b·∫°n ch·∫°y b√¨nh th∆∞·ªùng v·ªõi framework quen thu·ªôc (LangChain, AutoGen, etc.)
2. Tracer automatically instrument c√°c LLM calls v√† tool invocations th√†nh OpenTelemetry spans[13][14]
3. Data ƒë∆∞·ª£c collect v√† g·ª≠i v·ªÅ Lightning Server
4. Server train model b·∫±ng RL algorithms (PPO, GRPO)[10][11]
5. Model ƒë√£ optimize ƒë∆∞·ª£c serve l·∫°i cho agent th√¥ng qua OpenAI-compatible API[6][1]

***

### 2. Markov Decision Process (MDP) Formulation

Agent Lightning model h√≥a agent execution nh∆∞ m·ªôt Markov Decision Process ƒë·ªÉ c√≥ th·ªÉ √°p d·ª•ng RL. ƒê√¢y l√† n·ªÅn t·∫£ng to√°n h·ªçc cho ph√©p framework ho·∫°t ƒë·ªông v·ªõi b·∫•t k·ª≥ agent n√†o.[15][2][3][1]

#### C√°c th√†nh ph·∫ßn MDP trong Agent Lightning:

**State (Tr·∫°ng th√°i)**: Snapshot hi·ªán t·∫°i c·ªßa agent execution, bao g·ªìm c√°c bi·∫øn m√¥ t·∫£ ƒë·∫ßy ƒë·ªß execution status[3][1]

**Action (H√†nh ƒë·ªông)**: Output ƒë∆∞·ª£c generate b·ªüi policy LLM, ƒë∆∞·ª£c d√πng ƒë·ªÉ update state[1][3]

**Reward (Ph·∫ßn th∆∞·ªüng)**: Signal ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa transition[16][3][1]

**Transition (Chuy·ªÉn ƒë·ªïi)**: Tuple (state, action, reward, next_state) m√¥ t·∫£ vi·ªác agent di chuy·ªÉn gi·ªØa c√°c states[17][3][1]

**MDP Flow trong Agent Lightning:**

```mermaid
stateDiagram-v2
    [*] --> InitialState: Agent starts
    
    InitialState --> State1: Action: LLM generates query
    note right of State1
        State: User question + Context
        Action: Generate SQL query
        Reward: 0 (intermediate)
    end note
    
    State1 --> State2: Action: Execute SQL
    note right of State2
        State: SQL query + DB schema
        Action: Run query on database
        Reward: 0 (intermediate)
    end note
    
    State2 --> State3: Action: Format result
    note right of State3
        State: Query results + User question
        Action: Generate natural language answer
        Reward: +1 (correct) or -1 (wrong)
    end note
    
    State3 --> [*]: Task complete
    
    State1 --> State1: Action: Retry query generation
    State2 --> State1: Action: Query error, regenerate
```

***

### 3. Unified Data Interface

ƒê√¢y l√† abstraction layer quan tr·ªçng gi√∫p Agent Lightning ho·∫°t ƒë·ªông v·ªõi b·∫•t k·ª≥ agent framework n√†o. Interface n√†y:[7][3][1]

- **Abstracts complexity** c·ªßa c√°c agent execution logic kh√°c nhau[3][1]
- **Transforms data** ƒë∆∞·ª£c collect trong agent execution th√†nh training trajectories[1][3]
- **Captures complete execution context** trong m·ªói state[1]
- **Enables clean decoupling** gi·ªØa agent execution v√† RL training[3][1]

**Quy tr√¨nh thu th·∫≠p d·ªØ li·ªáu:**

```mermaid
sequenceDiagram
    participant Agent as Your Agent
    participant Tracer as OpenTelemetry Tracer
    participant Store as Lightning Store
    participant Trainer as Training Engine
    
    Agent->>Tracer: Execute agent workflow
    Note over Agent,Tracer: Agent runs normally
    
    Tracer->>Tracer: Instrument LLM calls
    Tracer->>Tracer: Capture tool invocations
    Tracer->>Tracer: Record rewards
    
    Tracer->>Store: Convert to Spans
    Note over Tracer,Store: Create unified format
    
    Store->>Store: Organize as tree structure
    Store->>Store: Extract state/action/reward
    
    Store->>Trainer: Provide Transitions
    Note over Store,Trainer: Training-ready data
    
    Trainer->>Trainer: Apply RL algorithm
    Trainer->>Agent: Update model
    Note over Trainer,Agent: Continuous improvement
```

***

### 4. LightningRL: Hierarchical RL Algorithm

LightningRL l√† thu·∫≠t to√°n RL ph√¢n c·∫•p ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho agent scenarios. ƒêi·ªÉm ƒë·∫∑c bi·ªát l√† **credit assignment module** cho ph√©p decompose trajectories th√†nh training transitions.[18][2][3][1]

#### V·∫•n ƒë·ªÅ Credit Assignment

Trong multi-turn agent interactions, l√†m sao bi·∫øt **b∆∞·ªõc n√†o** trong chu·ªói h√†nh ƒë·ªông ƒë√£ contribute v√†o k·∫øt qu·∫£ cu·ªëi c√πng?[19][20]

**V√≠ d·ª•:** Agent th·ª±c hi·ªán 10 b∆∞·ªõc ƒë·ªÉ gi·∫£i quy·∫øt m·ªôt b√†i to√°n:
- B∆∞·ªõc 3 v√† 7 r·∫•t quan tr·ªçng d·∫´n ƒë·∫øn success
- B∆∞·ªõc 1, 5, 9 √≠t contribute h∆°n
- C√°c b∆∞·ªõc c√≤n l·∫°i l√† neutral

‚Üí Credit assignment module ph·∫£i ph√¢n ph·ªëi reward t·ª´ final outcome v·ªÅ t·ª´ng b∆∞·ªõc m·ªôt c√°ch h·ª£p l√Ω[17][19][3]

**Credit Assignment v√† Integration v·ªõi Single-turn RL:**

```mermaid
graph TD
    A[Agent Trajectory<br/>Multiple LLM calls] --> B[Episode Level Reward<br/>Final task outcome]
    
    B --> C[Credit Assignment Module]
    
    C --> D[Strategy Selection]
    
    D --> E[Identical Assignment<br/>Equal reward for all steps]
    D --> F[Turn-based Assignment<br/>Weight by relevance]
    D --> G[Sophisticated Methods<br/>Value-based estimation]
    
    E --> H[Distributed Rewards<br/>r‚ÇÅ, r‚ÇÇ, r‚ÇÉ, ..., r‚Çô]
    F --> H
    G --> H
    
    H --> I[Single-turn RL Algorithm<br/>PPO/GRPO at token level]
    
    I --> J[Model Update<br/>Fine-tune LLM weights]
    
    style A fill:#e1f5ff
    style C fill:#ffe1e1
    style I fill:#fff4e1
    style J fill:#e1ffe1
```

#### Chi·∫øn l∆∞·ª£c Credit Assignment

Agent Lightning h·ªó tr·ª£ nhi·ªÅu strategies:[21][19][3]

**1. Identical Assignment**: M·ªçi LLM invocation nh·∫≠n reward b·∫±ng nhau[10][16]

**2. Turn-based v·ªõi Relevance Factor**: Non-terminal turns nh·∫≠n combination c·ªßa immediate reward v√† weighted final reward[19]

**3. Sophisticated Methods**: Value function estimation, temporal difference learning[19]

#### Integration v·ªõi Single-turn RL

Sau khi credit assignment, m·ªói LLM invocation ƒë∆∞·ª£c treat nh∆∞ independent single-turn RL problem:[21][3]
1. Credit assignment ph√¢n ph·ªëi episode-level reward v·ªÅ t·ª´ng step
2. M·ªói step tr·ªü th√†nh m·ªôt training sample
3. Apply existing efficient single-turn RL algorithms (PPO, GRPO)[11][22][10]
4. Update model weights based on assigned credits

***

## So s√°nh: Traditional Approach vs Agent Lightning

### Traditional Approach
- Tightly couple RL training v·ªõi agent code[2][1]
- Ph·∫£i rewrite cho m·ªói framework[2][1]
- Sequence concatenation v·ªõi masking ph·ª©c t·∫°p[2][1]
- Kh√≥ maintain v√† scale[2][1]

### Agent Lightning Approach
- Complete decoupling gi·ªØa agent v√† training[2][3][1]
- G·∫ßn nh∆∞ zero code modifications[5][4][1][2]
- Works v·ªõi b·∫•t k·ª≥ framework n√†o[4][7][1]
- Independent scaling (CPU rollout + GPU training)[8][12]

***

## V√≠ d·ª• th·ª±c t·∫ø: Integrate Agent Lightning

### B∆∞·ªõc 1: Agent code g·ªëc (LangChain)

```python
from langchain.agents import create_react_agent
from langchain_openai import ChatOpenAI
from langchain.tools import Tool

# Your existing agent - NO CHANGES NEEDED
llm = ChatOpenAI(model="gpt-4")
tools = [
    Tool(name="Calculator", func=calculator_func, description="Do math"),
    Tool(name="Search", func=search_func, description="Search web")
]

agent = create_react_agent(llm, tools)
```

### B∆∞·ªõc 2: Add Agent Lightning

```python
from agentlightning import LightningClient, OtelTracer

# Create Lightning Client
client = LightningClient(
    server_url="http://localhost:8000",
    rollout_id="my_rollout"
)

# Add tracer for automatic instrumentation
tracer = OtelTracer(enable_otel=True)

# Wrap your agent execution
async with tracer.trace_context(
    name="langchain-agent",
    store=client.store,
    rollout_id=client.rollout_id,
    attempt_id="attempt_1"
):
    result = agent.invoke({"input": "What's 25 * 17?"})
    
    # Emit reward based on correctness
    if verify_result(result):
        tracer.emit_reward(reward=1.0)
    else:
        tracer.emit_reward(reward=-1.0)
```

### B∆∞·ªõc 3: Configure training

```python
from agentlightning.trainer import Trainer
from agentlightning.algorithm.verl import VERL

algorithm = VERL(config={
    "algorithm": {
        "adv_estimator": "grpo",
        "use_kl_in_reward": False,
    },
    "data": {
        "train_batch_size": 32,
        "max_prompt_length": 4096,
        "max_response_length": 2048,
    }
})

trainer = Trainer(algorithm=algorithm, n_workers=4)
trainer.fit(agent=your_agent, backend="http://localhost:8000")
```

**ƒê√≥ l√† t·∫•t c·∫£!** Agent c·ªßa b·∫°n gi·ªù ƒë√£ c√≥ th·ªÉ h·ªçc v√† c·∫£i thi·ªán t·ª´ experience.[5][4][8][1]

***

## Experiments v√† Results

Agent Lightning ƒë√£ ƒë∆∞·ª£c test tr√™n nhi·ªÅu real-world tasks:[7][1][2]

**Text-to-SQL (Spider)**: LangGraph SQL agent v·ªõi selective optimization, significant accuracy improvement[12]

**Retrieval-Augmented Generation**: Optimize retrieval v√† answer generation, stable continuous improvements[1][2]

**Math Tool-Use (Calc-X)**: RL on correctness rewards, effective credit assignment trong multi-step reasoning[12]

**Key Findings**: Stable training, continuous improvement, scalable to production[7][2][1]

***

## Khi n√†o n√™n s·ª≠ d·ª•ng Agent Lightning?

### ‚úÖ Suitable for:

- Production agents c·∫ßn continuous improvement
- Complex multi-turn workflows v·ªõi tool use
- Khi b·∫°n ƒë√£ c√≥ agent code working v√† kh√¥ng mu·ªën rewrite

### ‚ùå C√≥ th·ªÉ kh√¥ng ph√π h·ª£p:

- Simple single-turn tasks
- Kh√¥ng c√≥ reward signal r√µ r√†ng
- Resource constraints (kh√¥ng c√≥ GPU infrastructure)

***

## Best Practices

**Reward Design**: S·ª≠ d·ª•ng verifiable rewards, combine terminal + intermediate rewards, start simple[16][17][19][12]

**Data Collection**: Collect diverse experiences, run parallel rollout workers, monitor quality[23][8][12]

**Training Strategy**: Start v·ªõi pre-trained models, use established algorithms (PPO/GRPO), monitor metrics[22][11][10]

---

## Roadmap v√† Future Work

Microsoft Research ƒëang plan nhi·ªÅu improvements:[24][21]

**ƒê√£ c√≥**: RL training v·ªõi PPO/GRPO, Automatic Prompt Optimization, Supervised Fine-tuning, Multi-agent training[9][11][10][4][5][7]

**ƒêang ph√°t tri·ªÉn**: Richer feedback mechanisms, Off-policy algorithms, Curriculum learning, Training-free optimizations, More sophisticated credit assignment[24][21]

---

## K·∫øt lu·∫≠n

Agent Lightning ƒë·∫°i di·ªán cho m·ªôt **paradigm shift** trong c√°ch ch√∫ng ta train v√† optimize AI agents. B·∫±ng c√°ch decoupling ho√†n to√†n agent execution kh·ªèi training system, s·ª≠ d·ª•ng MDP formulation v√† unified data interface, √°p d·ª•ng hierarchical RL v·ªõi intelligent credit assignment, v√† leverage observability frameworks, framework n√†y enables developers ƒë·ªÉ transform existing agents th√†nh learning systems v·ªõi minimal effort.[14][18][13][6][4][5][3][2][1]

**Cho beginner developers**: Agent Lightning lower barrier to entry cho agent optimization. B·∫°n kh√¥ng c·∫ßn ph·∫£i l√† RL expert ƒë·ªÉ benefit t·ª´ reinforcement learning cho LLMs.

**Cho experienced engineers**: Framework cung c·∫•p flexibility v√† extensibility ƒë·ªÉ customize theo specific needs.

***

## Resources

**Official Links**:
- GitHub: https://github.com/microsoft/agent-lightning
- Research Paper: https://arxiv.org/abs/2508.03680
- Documentation: https://microsoft.github.io/agent-lightning/
- Microsoft Research: https://www.microsoft.com/en-us/research/project/agent-lightning/

**Getting Started**:
```bash
pip install agentlightning
git clone https://github.com/microsoft/agent-lightning.git
```

***

Agent Lightning m·ªü ra possibilities m·ªõi cho AI agent development. Thay v√¨ accept agents with static capabilities, ch√∫ng ta gi·ªù c√≥ tools ƒë·ªÉ build agents that **learn, adapt, and improve** t·ª´ real-world experiences - v√† quan tr·ªçng nh·∫•t, l√†m ƒëi·ªÅu n√†y **without rewriting existing code**.[4][5][2][1]

N·∫øu b·∫°n ƒëang build AI agents, ƒë√¢y l√† th·ªùi ƒëi·ªÉm tuy·ªát v·ªùi ƒë·ªÉ explore Agent Lightning v√† xem n√≥ c√≥ th·ªÉ elevate agent capabilities c·ªßa b·∫°n nh∆∞ th·∫ø n√†o!c·ªßa b·∫°n nh∆∞ th·∫ø n√†o!

[1](https://arxiv.org/html/2508.03680)
[2](https://arxiv.org/abs/2508.03680)
[3](https://arxiv.org/html/2508.03680v1)
[4](https://microsoft.github.io/agent-lightning/latest/)
[5](https://github.com/microsoft/agent-lightning)
[6](https://www.microsoft.com/en-us/research/project/agent-lightning/)
[7](https://huggingface.co/papers/2508.03680)
[8](https://www.theunwindai.com/p/train-ai-agents-with-rl-no-code-changes)
[9](https://aiengineering.beehiiv.com/p/train-ai-agents-with-rl-agent-lightning-from-microsoft)
[10](https://microsoft.github.io/agent-lightning/stable/algorithm-zoo/verl/)
[11](https://github.com/volcengine/verl)
[12](https://joshuaberkowitz.us/blog/github-repos-8/agent-lightning-decoupled-rl-training-for-any-ai-agent-975)
[13](https://microsoft.github.io/agent-lightning/stable/tutorials/traces/)
[14](https://docs.koog.ai/opentelemetry-support/)
[15](https://www.geeksforgeeks.org/machine-learning/markov-decision-process/)
[16](https://news.aibase.com/news/22378)
[17](https://www.marktechpost.com/2025/10/29/microsoft-releases-agent-lightning-a-new-ai-framework-that-enables-reinforcement-learning-rl-based-training-of-llms-for-any-ai-agent/)
[18](https://chatpaper.com/paper/173395)
[19](https://hlfshell.ai/posts/multi-turn-credit-assignment/)
[20](https://discovery.ucl.ac.uk/id/eprint/10211495/1/Pignatelli_10211495_Thesis.pdf)
[21](https://www.youtube.com/watch?v=SNPZy3sAv2Q)
[22](https://verl.readthedocs.io/en/latest/algo/grpo.html)
[23](https://lifetime.fi/blog/Lightning)
[24](https://nielsberglund.com/post/2025-11-02-interesting-stuff---week-44-2025/)
[25](https://docs.langchain.com/langsmith/autogen-integration)
[26](https://www.linkedin.com/posts/skphd_agent-lightning-activity-7360485368950870016-qK7a)
[27](https://www.facebook.com/groups/ainocode/posts/846923091419146/)
[28](https://developer.nvidia.com/blog/building-an-interactive-ai-agent-for-lightning-fast-machine-learning-tasks/)
[29](https://www.linkedin.com/pulse/agent-lightning-supercharging-ai-agents-engineers-khanchandani-4jqxc)
[30](https://news.ycombinator.com/item?id=45706729)
[31](https://pub.towardsai.net/agent-lightning-revolutionizing-ai-agent-training-with-reinforcement-learning-885bc78daa2c)
[32](https://www.alphaxiv.org/overview/2508.03680v1)
[33](https://prezi.com/p/itmehoebt1jw/agent-lightning-training-ai-agents-with-reinforcement-learning/)
[34](https://www.reddit.com/r/singularity/comments/1oj95ji/agent_lightning_train_any_ai_agents_with/)
[35](https://introml.mit.edu/notes/mdp.html)
[36](https://gibberblot.github.io/rl-notes/single-agent/MDPs.html)
[37](https://en.wikipedia.org/wiki/Markov_decision_process)
[38](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-observability)
[39](https://outshift.cisco.com/blog/ai-observability-multi-agent-systems-opentelemetry)
[40](https://lightning.ai/lightning-ai/environments/training-a-coding-agent-with-verl)
[41](https://opentelemetry.io/blog/2025/ai-agent-observability/)
[42](https://opentelemetry.io/docs/concepts/observability-primer/)
[43](https://github.com/langfengQ/verl-agent)
[44](https://mcp-builder.ai/mcp-training-center/build-ai-agent-langchain-mcp)
[45](https://www.codecademy.com/article/agentic-ai-with-langchain-langgraph)
[46](https://github.com/OpenAccess-AI-Collective/axolotl/issues/128)
[47](https://docs.langchain.com/oss/python/langchain/agents)
[48](https://github.com/NVIDIA/NeMo/issues/9664)
[49](https://dilipa.github.io/papers/info_credit.pdf)
[50](https://dev.to/pavanbelagatti/build-a-real-time-news-ai-agent-using-langchain-in-just-a-few-steps-4d60)
[51](https://openreview.net/pdf/6718149705ee4f6bfffcce1d48be9f6ad3679247.pdf)
[52](https://www.langchain.com)
[53](https://arxiv.org/html/2509.06733v1)
[54](https://arxiv.org/html/2510.00023v1)
[55](https://arxiv.org/html/2510.10991v1)
[56](https://arxiv.org/abs/2401.03568)
[57](https://arxiv.org/html/2508.11553v1)
[58](https://arxiv.org/pdf/2404.11584.pdf)
[59](https://www.facebook.com/asifrazzaq1988/photos/microsoft-releases-agent-lightning-a-new-ai-framework-that-enables-reinforcement/3306146439523410/)
[60](https://arxiv.org/html/2510.16720v1)
[61](https://arxiv.org/pdf/2407.01502.pdf)
[62](https://arxiv.org/html/2509.02547v1)
[63](https://www.mathworks.com/help/reinforcement-learning/ug/train-reinforcement-learning-agent-in-mdp-environment.html)
[64](https://apxml.com/courses/intro-to-reinforcement-learning/chapter-2-markov-decision-processes-mdps/reward-functions)
[65](https://pantelis.github.io/aiml-common/lectures/mdp/mdp-intro/)
[66](https://www.reddit.com/r/machinelearningnews/comments/1ojhlma/microsoft_releases_agent_lightning_a_new_ai/)
[67](https://www.linkedin.com/posts/mayank-sultania-2ab9a514b_agentlightning-aiagents-tech2025-activity-7367420824628183041-kNvm)
[68](https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning)
[69](https://blog.vllm.ai/2025/10/22/agent-lightning.html)
[70](https://inst.eecs.berkeley.edu/~cs188/textbook/mdp/markov-decision-processes.html)
[71](https://www.marktechpost.com/2025/08/31/step-by-step-guide-to-ai-agent-development-using-microsoft-agent-lightning/)
[72](https://www.reinforcementlearningpath.com/markov-decision-process-mdp-application-1/)